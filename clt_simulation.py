# -*- coding: utf-8 -*-
"""CLT Simulation

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NU-mHd-Qx3ePDEjoBNzkzzwcWcLFUc9w

```
# Credit: Alwan R. Subian, Tangerang Selatan (2023).
```



Central Limit Theorem (CLT) is a fundamental concept in statistics and probability theory. It states that the sampling distribution of the mean of a sufficiently large number of independent and identically distributed (i.i.d.) random variables approaches a normal distribution, regardless of the shape of the original population distribution.

In simpler terms, the Central Limit Theorem says that if you take multiple random samples from a population and calculate the mean of each sample, the distribution of those sample means will tend to follow a bell-shaped curve (a normal distribution), even if the original population's distribution is not normal.

Key points about the Central Limit Theorem:

* The CLT applies to the sampling distribution of the mean (or the sum) of random samples, not to the distribution of individual data points in the population.

* The individual samples must be randomly drawn and have the same underlying distribution (i.i.d.). Random sampling ensures that each observation is independent of others.

* As the sample size increases, the shape of the sampling distribution of the mean becomes closer to a normal distribution, regardless of the shape of the original population.

* The CLT is a fundamental concept in inferential statistics. It allows statisticians to make inferences about population parameters (e.g., population mean) based on the sample mean and its distribution.

* The larger the sample size, the better the approximation to a normal distribution. However, even with relatively small sample sizes (e.g., n > 30), the approximation to normality is often good enough for practical purposes.

# Generating Population
"""

import random
import pandas as pd
import numpy as np
from scipy import stats

# Set the value of n (number of random numbers to generate)
n = 100000

# Generate n random numbers between 0 and 1 (you can adjust the range as needed)
random_numbers = [random.randint(1,1000) for x in range(n)]

# Create a DataFrame from the generated random numbers
df = pd.DataFrame({'Random Numbers': random_numbers})

# Print the DataFrame
print(df)

print(np.std(df)*2)

"""# Histogram of Population
Let's check the population's distribution base on histogram
"""

import matplotlib.pyplot as plt
# Create histogram for population
plt.hist(df['Random Numbers'], bins=100, edgecolor='black')
plt.xlabel('Averages')
plt.ylabel('Frequency')
plt.title('Histogram of Population')
plt.show()

"""# Do The Sampling
At this section, random numbers are taken from population and we count the mean from those random numbers.
"""

# Make a dummy dataframe
av = pd.DataFrame()
# Here number of iterations is set
num_iterations = 5000

# Loop to calculate the averages
for i in range(num_iterations):
    random_sample = df.sample(n=500) # n is number of sample taken to draw mean
    #print(random_sample)
    average = random_sample['Random Numbers'].mean()
    av = av.append({'Average': average}, ignore_index=True)

"""# Sample Distribution
We are going to see whether the sample distributon is normal or not base on graph and test
"""

# Make a histogram for the DataFrame av
plt.axvline(np.mean(df["Random Numbers"]), color='red', linestyle='solid', linewidth=3, label='Population Mean')
plt.axvline(np.mean(av["Average"]), color='green', linestyle='solid', linewidth=3, label='Sample Mean')
plt.hist(av['Average'], bins=100, edgecolor='black')
plt.xlabel('Averages')
plt.ylabel('Frequency')
plt.title('Histogram of Averages')
plt.show()

import statsmodels.api as sm
# Create a Q-Q plot
sm.qqplot(av["Average"], line='s')  # 's' stands for standardized line

# Set plot labels and title
plt.xlabel("Theoretical Quantiles")
plt.ylabel("Sample Quantiles")
plt.title("Q-Q Plot")

# Display the plot
plt.show()

# Shapiro-Wilk Test
shapiro_statistic, shapiro_p_value = stats.shapiro(av["Average"])
print("Shapiro-Wilk Test:")
print("Test Statistic:", shapiro_statistic)
print("P-Value:", shapiro_p_value)
alpha = 0.05
print("alpha = 0.05")

# Interpret the Shapiro-Wilk test result

if shapiro_p_value > alpha:
  print("The data follows a normal distribution (fail to reject H0).")
else:
  print("The data does not follow a normal distribution (reject H0).")

# Anderson-Darling Test
anderson_statistic, anderson_critical_values, anderson_significance_levels = stats.anderson(av["Average"], dist='norm')
print("\nAnderson-Darling Test:")
print("Test Statistic:", anderson_statistic)
print("Critical Values:", anderson_critical_values)
print("Significance Levels:", anderson_significance_levels)

# Interpret the Anderson-Darling test result
for i, level in enumerate(anderson_significance_levels):
  if anderson_statistic < anderson_critical_values[i]:
      print(f"The data follows a normal distribution at {level}% significance level.")
  else:
      print(f"The data does not follow a normal distribution at {level}% significance level.")

"""# Z-test
We  want to check whether the statistic (in this case mean) is same as the population or not.
"""

from scipy import stats

# Known population standard deviations
population_std1 = np.std(df)
population_std2 = population_std1

# Sample sizes
n1 = len(df)
n2 = len(av)

# Sample means
sample_mean1 = np.mean(df["Random Numbers"])
sample_mean2 = np.mean(av["Average"])

# Perform the Z-test
z_statistic = (sample_mean1 - sample_mean2) / np.sqrt((population_std1**2 / n1) + (population_std2**2 / n2))

# Calculate the p-value using the cumulative distribution function (CDF)
p_value = 2 * (1 - stats.norm.cdf(np.abs(z_statistic)))

# Print the Z-statistic and p-value
print("Z-Statistic:", z_statistic)
print("P-Value:", p_value)

# Check if the result is significant at a 95% confidence level
alpha = 0.05
if p_value < alpha:
    print("The difference between the two groups is significant.")
else:
    print("There is no significant difference between the two groups.")
